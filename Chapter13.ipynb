{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter13.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naomori/codexa_LinearRegression_Introduction/blob/master/Chapter13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXvjMFAFmT5N",
        "colab_type": "text"
      },
      "source": [
        "# コスト関数と最急降下法\n",
        "\n",
        "\n",
        "$\\hat{y} = w_{1}x + w_{0}$\n",
        "\n",
        "$J(w_0, w_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y_i}-y_i)^2$\n",
        "\n",
        "$w_0 := w_0 - \\alpha\\frac{\\partial}{\\partial w_0}J(w_0, w_1)$\n",
        "\n",
        "$w_1 := w_1 - \\alpha\\frac{\\partial}{\\partial w_1}J(w_0, w_1)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5c57LYumoFG",
        "colab_type": "text"
      },
      "source": [
        "## 偏微分から見ていきます\n",
        "\n",
        "$w_0$ の右側の偏微分の式のみを取り出し、\n",
        "$J(w_0, w_1)$ の式を置き換えます。\n",
        "\n",
        "$\\frac{\\partial}{\\partial w_0}J(w_0, w_1) = \\frac{\\partial}{\\partial w_0} \\times \\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y_i}-y_i)^2$\n",
        "\n",
        "$\\hat{y}$ も代入する。\n",
        "\n",
        "$\\frac{\\partial}{\\partial w_0}J(w_0, w_1) = \\frac{\\partial}{\\partial w_0} \\times \\frac{1}{2m}\\sum_{i=1}^{m}(w_1x+w_0-y_i)^2$\n",
        "\n",
        "右側には $w_0$ を含んでいるので、$w_0$で偏微分できます。\n",
        "\n",
        "$\\frac{\\partial}{\\partial w_0}J(w_0, w_1) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y_i}-y_i)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKnATV-NoZqj",
        "colab_type": "text"
      },
      "source": [
        "## $w_1$ の方も見てみると、$w_0$ と同じだが、$w_1$ にかかっている $x_i$ に注意\n",
        "\n",
        "\n",
        "$\\frac{\\partial}{\\partial w_1}J(w_0, w_1) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y_i}-y_i) \\times x_i$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6yh7Dl2o-1e",
        "colab_type": "text"
      },
      "source": [
        "## 線形回帰（Linear Regression）と最急降下法（Gradient Descent）\n",
        "\n",
        "$\\hat{y} = w_{1}x + w_{0}$\n",
        "\n",
        "$J(w_0, w_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y_i}-y_i)^2$\n",
        "\n",
        "**下記は同時更新であることに注意**\n",
        "\n",
        "$w_0 := w_0 - \\alpha\\frac{\\partial}{\\partial w_0}J(w_0, w_1)$\n",
        "\n",
        "$w_1 := w_1 - \\alpha\\frac{\\partial}{\\partial w_1}J(w_0, w_1)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dpY9nKto4uj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}